{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.0.1.post2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (0.2.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.16.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.12.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision) (5.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1a6163dcdc773d5c64048b5b3a372736fac4640c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4d4c24f6f7483fdbd780908596cf490060d8a648"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fae0172ee893f14999870cd28ef5e3117bcef689"
   },
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "67216fd8696728ac17b8df4cb8a91ff49a91fda1"
   },
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "     def __init__(self, file_path, \n",
    "                 transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n",
    "                     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "                ):\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df.columns) == n_pixels:\n",
    "            # test data\n",
    "            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = None\n",
    "        else:\n",
    "            # training data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = torch.from_numpy(df.iloc[:,0].values)\n",
    "            \n",
    "        self.transform = transform\n",
    "    \n",
    "     def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "     def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a82bcfe9d716843893d2b3b140dcca5b61e7f61b"
   },
   "source": [
    "Random Rotation Transformation\n",
    "\n",
    "\n",
    "Randomly rotate the image. Available in upcoming torchvision but not now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6f163d6cf92638c7cccb99814513638d05d54637"
   },
   "outputs": [],
   "source": [
    "class RandomRotation(object):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/tree/master/torchvision/transforms\n",
    "    Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = np.random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "        \"\"\"\n",
    "        \n",
    "        def rotate(img, angle, resample=False, expand=False, center=None):\n",
    "            \"\"\"Rotate the image by angle and then (optionally) translate it by (n_columns, n_rows)\n",
    "            Args:\n",
    "            img (PIL Image): PIL Image to be rotated.\n",
    "            angle ({float, int}): In degrees degrees counter clockwise order.\n",
    "            resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "            expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output image to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "            center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "            \"\"\"\n",
    "                \n",
    "            return img.rotate(angle, resample, expand, center)\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return rotate(img, angle, self.resample, self.expand, self.center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fbfccadca6d4ecae1a4bb6043a8aa2caaa1fe05"
   },
   "source": [
    "Random Vertical and Horizontal Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2a595138822303c1772fec7cdfd98f54679630ec"
   },
   "outputs": [],
   "source": [
    "class RandomShift(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_params(shift):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n",
    "\n",
    "        return hshift, vshift \n",
    "    def __call__(self, img):\n",
    "        hshift, vshift = self.get_params(self.shift)\n",
    "        \n",
    "        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73f97c56221e43bfa15fc7b3e0ee5d929abe0606"
   },
   "source": [
    "Load the Data into Tensors\n",
    "\n",
    "For the training set, apply random rotation within the range of (-45, 45) degrees, shift by (-3, 3) pixels and normalize pixel values to [-1, 1]. For the test set, only apply nomalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1ee9585424b77d3c642a8a1fdf3dd192f140da88"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_test = len(test_df)\n",
    "n_pixels = len(test_df.columns)\n",
    "train_dataset = mydata('../input/train.csv', transform= transforms.Compose(\n",
    "                            [transforms.ToPILImage(), RandomRotation(degrees=20), RandomShift(3),\n",
    "                             transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "test_dataset = mydata('../input/test.csv', )\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c262e65edea82ce3f89b71dcd7190f5f1cd6ebfa"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "          \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p = 0.25),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1,padding=1),\n",
    "             nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "             nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         \n",
    "            nn.Dropout(p = 0.25),\n",
    "        )\n",
    "          \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.25),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.25),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "0536f2ae2a0248df71a3c829833dc926fd9abada"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "253ac1a816dcf18ebd84dc5c3b29e53cef2f960e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Dropout(p=0.25)\n",
      "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Dropout(p=0.25)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=6272, out_features=512, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.25)\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "44084377ac01a36cc667c47488d63aaf574e8a59"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    exp_lr_scheduler.step()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f9fc523dd2d0dcc2cba152ae39bbd006cdec16bf"
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data,volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "        \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ee412610c8e64229bc300d51aac08f084f8bde12",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/42000 (15%)]\tLoss: 0.543324\n",
      "Train Epoch: 0 [12800/42000 (30%)]\tLoss: 0.432182\n",
      "Train Epoch: 0 [19200/42000 (46%)]\tLoss: 0.518782\n",
      "Train Epoch: 0 [25600/42000 (61%)]\tLoss: 0.396095\n",
      "Train Epoch: 0 [32000/42000 (76%)]\tLoss: 0.254647\n",
      "Train Epoch: 0 [38400/42000 (91%)]\tLoss: 0.352039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.1654, Accuracy: 39883/42000 (94.000%)\n",
      "\n",
      "Train Epoch: 1 [6400/42000 (15%)]\tLoss: 0.185210\n",
      "Train Epoch: 1 [12800/42000 (30%)]\tLoss: 0.332314\n",
      "Train Epoch: 1 [19200/42000 (46%)]\tLoss: 0.113177\n",
      "Train Epoch: 1 [25600/42000 (61%)]\tLoss: 0.247065\n",
      "Train Epoch: 1 [32000/42000 (76%)]\tLoss: 0.150347\n",
      "Train Epoch: 1 [38400/42000 (91%)]\tLoss: 0.255824\n",
      "\n",
      "Average loss: 0.1410, Accuracy: 40181/42000 (95.000%)\n",
      "\n",
      "Train Epoch: 2 [6400/42000 (15%)]\tLoss: 0.172260\n",
      "Train Epoch: 2 [12800/42000 (30%)]\tLoss: 0.128802\n",
      "Train Epoch: 2 [19200/42000 (46%)]\tLoss: 0.310824\n",
      "Train Epoch: 2 [25600/42000 (61%)]\tLoss: 0.160262\n",
      "Train Epoch: 2 [32000/42000 (76%)]\tLoss: 0.141309\n",
      "Train Epoch: 2 [38400/42000 (91%)]\tLoss: 0.069232\n",
      "\n",
      "Average loss: 0.1213, Accuracy: 40471/42000 (96.000%)\n",
      "\n",
      "Train Epoch: 3 [6400/42000 (15%)]\tLoss: 0.154936\n",
      "Train Epoch: 3 [12800/42000 (30%)]\tLoss: 0.049069\n",
      "Train Epoch: 3 [19200/42000 (46%)]\tLoss: 0.105068\n",
      "Train Epoch: 3 [25600/42000 (61%)]\tLoss: 0.028185\n",
      "Train Epoch: 3 [32000/42000 (76%)]\tLoss: 0.056211\n",
      "Train Epoch: 3 [38400/42000 (91%)]\tLoss: 0.057737\n",
      "\n",
      "Average loss: 0.0848, Accuracy: 40965/42000 (97.000%)\n",
      "\n",
      "Train Epoch: 4 [6400/42000 (15%)]\tLoss: 0.060651\n",
      "Train Epoch: 4 [12800/42000 (30%)]\tLoss: 0.145659\n",
      "Train Epoch: 4 [19200/42000 (46%)]\tLoss: 0.162396\n",
      "Train Epoch: 4 [25600/42000 (61%)]\tLoss: 0.127539\n",
      "Train Epoch: 4 [32000/42000 (76%)]\tLoss: 0.083320\n",
      "Train Epoch: 4 [38400/42000 (91%)]\tLoss: 0.063499\n",
      "\n",
      "Average loss: 0.0827, Accuracy: 40917/42000 (97.000%)\n",
      "\n",
      "Train Epoch: 5 [6400/42000 (15%)]\tLoss: 0.393520\n",
      "Train Epoch: 5 [12800/42000 (30%)]\tLoss: 0.113096\n",
      "Train Epoch: 5 [19200/42000 (46%)]\tLoss: 0.103005\n",
      "Train Epoch: 5 [25600/42000 (61%)]\tLoss: 0.085652\n",
      "Train Epoch: 5 [32000/42000 (76%)]\tLoss: 0.366926\n",
      "Train Epoch: 5 [38400/42000 (91%)]\tLoss: 0.189826\n",
      "\n",
      "Average loss: 0.0884, Accuracy: 40867/42000 (97.000%)\n",
      "\n",
      "Train Epoch: 6 [6400/42000 (15%)]\tLoss: 0.052590\n",
      "Train Epoch: 6 [12800/42000 (30%)]\tLoss: 0.055128\n",
      "Train Epoch: 6 [19200/42000 (46%)]\tLoss: 0.113309\n",
      "Train Epoch: 6 [25600/42000 (61%)]\tLoss: 0.130820\n",
      "Train Epoch: 6 [32000/42000 (76%)]\tLoss: 0.112333\n",
      "Train Epoch: 6 [38400/42000 (91%)]\tLoss: 0.055279\n",
      "\n",
      "Average loss: 0.0743, Accuracy: 41037/42000 (97.000%)\n",
      "\n",
      "Train Epoch: 7 [6400/42000 (15%)]\tLoss: 0.048870\n",
      "Train Epoch: 7 [12800/42000 (30%)]\tLoss: 0.123928\n",
      "Train Epoch: 7 [19200/42000 (46%)]\tLoss: 0.028491\n",
      "Train Epoch: 7 [25600/42000 (61%)]\tLoss: 0.042349\n",
      "Train Epoch: 7 [32000/42000 (76%)]\tLoss: 0.094231\n",
      "Train Epoch: 7 [38400/42000 (91%)]\tLoss: 0.029908\n",
      "\n",
      "Average loss: 0.0438, Accuracy: 41431/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 8 [6400/42000 (15%)]\tLoss: 0.012625\n",
      "Train Epoch: 8 [12800/42000 (30%)]\tLoss: 0.011161\n",
      "Train Epoch: 8 [19200/42000 (46%)]\tLoss: 0.026592\n",
      "Train Epoch: 8 [25600/42000 (61%)]\tLoss: 0.089342\n",
      "Train Epoch: 8 [32000/42000 (76%)]\tLoss: 0.049539\n",
      "Train Epoch: 8 [38400/42000 (91%)]\tLoss: 0.049822\n",
      "\n",
      "Average loss: 0.0444, Accuracy: 41431/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 9 [6400/42000 (15%)]\tLoss: 0.091203\n",
      "Train Epoch: 9 [12800/42000 (30%)]\tLoss: 0.021525\n",
      "Train Epoch: 9 [19200/42000 (46%)]\tLoss: 0.056384\n",
      "Train Epoch: 9 [25600/42000 (61%)]\tLoss: 0.063506\n",
      "Train Epoch: 9 [32000/42000 (76%)]\tLoss: 0.057618\n",
      "Train Epoch: 9 [38400/42000 (91%)]\tLoss: 0.005006\n",
      "\n",
      "Average loss: 0.0430, Accuracy: 41454/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 10 [6400/42000 (15%)]\tLoss: 0.032381\n",
      "Train Epoch: 10 [12800/42000 (30%)]\tLoss: 0.013059\n",
      "Train Epoch: 10 [19200/42000 (46%)]\tLoss: 0.126940\n",
      "Train Epoch: 10 [25600/42000 (61%)]\tLoss: 0.037372\n",
      "Train Epoch: 10 [32000/42000 (76%)]\tLoss: 0.014474\n",
      "Train Epoch: 10 [38400/42000 (91%)]\tLoss: 0.021017\n",
      "\n",
      "Average loss: 0.0366, Accuracy: 41526/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 11 [6400/42000 (15%)]\tLoss: 0.047347\n",
      "Train Epoch: 11 [12800/42000 (30%)]\tLoss: 0.160750\n",
      "Train Epoch: 11 [19200/42000 (46%)]\tLoss: 0.222200\n",
      "Train Epoch: 11 [25600/42000 (61%)]\tLoss: 0.032746\n",
      "Train Epoch: 11 [32000/42000 (76%)]\tLoss: 0.057543\n",
      "Train Epoch: 11 [38400/42000 (91%)]\tLoss: 0.067586\n",
      "\n",
      "Average loss: 0.0372, Accuracy: 41519/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 12 [6400/42000 (15%)]\tLoss: 0.035959\n",
      "Train Epoch: 12 [12800/42000 (30%)]\tLoss: 0.109315\n",
      "Train Epoch: 12 [19200/42000 (46%)]\tLoss: 0.142426\n",
      "Train Epoch: 12 [25600/42000 (61%)]\tLoss: 0.094220\n",
      "Train Epoch: 12 [32000/42000 (76%)]\tLoss: 0.017032\n",
      "Train Epoch: 12 [38400/42000 (91%)]\tLoss: 0.010969\n",
      "\n",
      "Average loss: 0.0367, Accuracy: 41528/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 13 [6400/42000 (15%)]\tLoss: 0.087012\n",
      "Train Epoch: 13 [12800/42000 (30%)]\tLoss: 0.190802\n",
      "Train Epoch: 13 [19200/42000 (46%)]\tLoss: 0.131251\n",
      "Train Epoch: 13 [25600/42000 (61%)]\tLoss: 0.075083\n",
      "Train Epoch: 13 [32000/42000 (76%)]\tLoss: 0.053391\n",
      "Train Epoch: 13 [38400/42000 (91%)]\tLoss: 0.096942\n",
      "\n",
      "Average loss: 0.0359, Accuracy: 41534/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 14 [6400/42000 (15%)]\tLoss: 0.124249\n",
      "Train Epoch: 14 [12800/42000 (30%)]\tLoss: 0.058162\n",
      "Train Epoch: 14 [19200/42000 (46%)]\tLoss: 0.111344\n",
      "Train Epoch: 14 [25600/42000 (61%)]\tLoss: 0.007370\n",
      "Train Epoch: 14 [32000/42000 (76%)]\tLoss: 0.021972\n",
      "Train Epoch: 14 [38400/42000 (91%)]\tLoss: 0.009504\n",
      "\n",
      "Average loss: 0.0358, Accuracy: 41533/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 15 [6400/42000 (15%)]\tLoss: 0.040002\n",
      "Train Epoch: 15 [12800/42000 (30%)]\tLoss: 0.135583\n",
      "Train Epoch: 15 [19200/42000 (46%)]\tLoss: 0.126358\n",
      "Train Epoch: 15 [25600/42000 (61%)]\tLoss: 0.210045\n",
      "Train Epoch: 15 [32000/42000 (76%)]\tLoss: 0.032098\n",
      "Train Epoch: 15 [38400/42000 (91%)]\tLoss: 0.066976\n",
      "\n",
      "Average loss: 0.0358, Accuracy: 41547/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 16 [6400/42000 (15%)]\tLoss: 0.095017\n",
      "Train Epoch: 16 [12800/42000 (30%)]\tLoss: 0.038356\n",
      "Train Epoch: 16 [19200/42000 (46%)]\tLoss: 0.011832\n",
      "Train Epoch: 16 [25600/42000 (61%)]\tLoss: 0.018901\n",
      "Train Epoch: 16 [32000/42000 (76%)]\tLoss: 0.026743\n",
      "Train Epoch: 16 [38400/42000 (91%)]\tLoss: 0.027575\n",
      "\n",
      "Average loss: 0.0347, Accuracy: 41553/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 17 [6400/42000 (15%)]\tLoss: 0.040578\n",
      "Train Epoch: 17 [12800/42000 (30%)]\tLoss: 0.095235\n",
      "Train Epoch: 17 [19200/42000 (46%)]\tLoss: 0.016343\n",
      "Train Epoch: 17 [25600/42000 (61%)]\tLoss: 0.013484\n",
      "Train Epoch: 17 [32000/42000 (76%)]\tLoss: 0.034345\n",
      "Train Epoch: 17 [38400/42000 (91%)]\tLoss: 0.036011\n",
      "\n",
      "Average loss: 0.0371, Accuracy: 41500/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 18 [6400/42000 (15%)]\tLoss: 0.148475\n",
      "Train Epoch: 18 [12800/42000 (30%)]\tLoss: 0.080540\n",
      "Train Epoch: 18 [19200/42000 (46%)]\tLoss: 0.089758\n",
      "Train Epoch: 18 [25600/42000 (61%)]\tLoss: 0.088579\n",
      "Train Epoch: 18 [32000/42000 (76%)]\tLoss: 0.027559\n",
      "Train Epoch: 18 [38400/42000 (91%)]\tLoss: 0.037604\n",
      "\n",
      "Average loss: 0.0339, Accuracy: 41560/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 19 [6400/42000 (15%)]\tLoss: 0.048183\n",
      "Train Epoch: 19 [12800/42000 (30%)]\tLoss: 0.005767\n",
      "Train Epoch: 19 [19200/42000 (46%)]\tLoss: 0.021030\n",
      "Train Epoch: 19 [25600/42000 (61%)]\tLoss: 0.095125\n",
      "Train Epoch: 19 [32000/42000 (76%)]\tLoss: 0.020292\n",
      "Train Epoch: 19 [38400/42000 (91%)]\tLoss: 0.011640\n",
      "\n",
      "Average loss: 0.0339, Accuracy: 41548/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 20 [6400/42000 (15%)]\tLoss: 0.116562\n",
      "Train Epoch: 20 [12800/42000 (30%)]\tLoss: 0.096152\n",
      "Train Epoch: 20 [19200/42000 (46%)]\tLoss: 0.181822\n",
      "Train Epoch: 20 [25600/42000 (61%)]\tLoss: 0.061852\n",
      "Train Epoch: 20 [32000/42000 (76%)]\tLoss: 0.007863\n",
      "Train Epoch: 20 [38400/42000 (91%)]\tLoss: 0.098629\n",
      "\n",
      "Average loss: 0.0338, Accuracy: 41551/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 21 [6400/42000 (15%)]\tLoss: 0.012919\n",
      "Train Epoch: 21 [12800/42000 (30%)]\tLoss: 0.020419\n",
      "Train Epoch: 21 [19200/42000 (46%)]\tLoss: 0.190470\n",
      "Train Epoch: 21 [25600/42000 (61%)]\tLoss: 0.012878\n",
      "Train Epoch: 21 [32000/42000 (76%)]\tLoss: 0.061249\n",
      "Train Epoch: 21 [38400/42000 (91%)]\tLoss: 0.010432\n",
      "\n",
      "Average loss: 0.0336, Accuracy: 41541/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 22 [6400/42000 (15%)]\tLoss: 0.016495\n",
      "Train Epoch: 22 [12800/42000 (30%)]\tLoss: 0.033514\n",
      "Train Epoch: 22 [19200/42000 (46%)]\tLoss: 0.010419\n",
      "Train Epoch: 22 [25600/42000 (61%)]\tLoss: 0.051396\n",
      "Train Epoch: 22 [32000/42000 (76%)]\tLoss: 0.025957\n",
      "Train Epoch: 22 [38400/42000 (91%)]\tLoss: 0.015292\n",
      "\n",
      "Average loss: 0.0332, Accuracy: 41559/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 23 [6400/42000 (15%)]\tLoss: 0.050509\n",
      "Train Epoch: 23 [12800/42000 (30%)]\tLoss: 0.054828\n",
      "Train Epoch: 23 [19200/42000 (46%)]\tLoss: 0.033076\n",
      "Train Epoch: 23 [25600/42000 (61%)]\tLoss: 0.084369\n",
      "Train Epoch: 23 [32000/42000 (76%)]\tLoss: 0.108329\n",
      "Train Epoch: 23 [38400/42000 (91%)]\tLoss: 0.053902\n",
      "\n",
      "Average loss: 0.0341, Accuracy: 41577/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 24 [6400/42000 (15%)]\tLoss: 0.177006\n",
      "Train Epoch: 24 [12800/42000 (30%)]\tLoss: 0.034911\n",
      "Train Epoch: 24 [19200/42000 (46%)]\tLoss: 0.058918\n",
      "Train Epoch: 24 [25600/42000 (61%)]\tLoss: 0.010950\n",
      "Train Epoch: 24 [32000/42000 (76%)]\tLoss: 0.011805\n",
      "Train Epoch: 24 [38400/42000 (91%)]\tLoss: 0.012075\n",
      "\n",
      "Average loss: 0.0327, Accuracy: 41562/42000 (98.000%)\n",
      "\n",
      "Train Epoch: 25 [6400/42000 (15%)]\tLoss: 0.054142\n",
      "Train Epoch: 25 [12800/42000 (30%)]\tLoss: 0.046831\n",
      "Train Epoch: 25 [19200/42000 (46%)]\tLoss: 0.077793\n",
      "Train Epoch: 25 [25600/42000 (61%)]\tLoss: 0.049287\n",
      "Train Epoch: 25 [32000/42000 (76%)]\tLoss: 0.179804\n",
      "Train Epoch: 25 [38400/42000 (91%)]\tLoss: 0.052233\n",
      "\n",
      "Average loss: 0.0336, Accuracy: 41580/42000 (99.000%)\n",
      "\n",
      "Train Epoch: 26 [6400/42000 (15%)]\tLoss: 0.031352\n",
      "Train Epoch: 26 [12800/42000 (30%)]\tLoss: 0.063896\n",
      "Train Epoch: 26 [19200/42000 (46%)]\tLoss: 0.054309\n",
      "Train Epoch: 26 [25600/42000 (61%)]\tLoss: 0.148639\n",
      "Train Epoch: 26 [32000/42000 (76%)]\tLoss: 0.052417\n",
      "Train Epoch: 26 [38400/42000 (91%)]\tLoss: 0.023552\n",
      "\n",
      "Average loss: 0.0329, Accuracy: 41549/42000 (98.000%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 27\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "4e67e0c1ddf14c8ec9897beeb1929a17e3904ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def prediciton(data_loader):\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            \n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.cpu().data.max(1, keepdim=True)[1]\n",
    "        test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "        \n",
    "    return test_pred\n",
    "test_pred = prediciton(test_loader)\n",
    "out_df = pd.DataFrame(np.c_[np.arange(1, len(test_dataset)+1)[:,None], test_pred.numpy()], \n",
    "                      columns=['ImageId', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f9bfc21615fb3483c9f74e42fbf8dc9a6056aefe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_pred = prediciton(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5406a9b0bc78a107266fbe87f7b82d9a31bd0405"
   },
   "outputs": [],
   "source": [
    "out_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
